Source code management system / Version Control System
================================================
Centralized VCS
Distributed VCS

Github --> Cloud based secured storage repository
Collaborate and version control

Git --> Desktop based application --> repository

To copy a repo to your local machine
===============================
Download the repository as a zip file
on the cmd, git clone followed by the url for the repo

Accessing an individual's repository
=============================
git fork --> forking a repo, creating a copy of an individual's repo in your account

Pull request --> To notify the changes to the author
Merge pull request

 Git commands
=============
git init --> To initiate a empty local repository
git remote add origin "url to your repo"
git pull origin main--> Pull the remore repo into local
vi file.py --> make changes to the file pulled from remote
git add file.py
git commit  -m "saved"
git status
git push origin master
You can accept or reject the merge pull request from the local/git in the remote/github 

Before committing, the data will be in staging area

# Versioning
git tag

git clean  -f  --> To remove untracked files from the local repo
git log   -n     --> Show the limited number of previous commits
git stash --> If you want to save a file without committing into git (stashed commit)
git checkout  -b   branch name --> to switch between the branch 
git config --> get and set configuration variables that controls all the aspects of git
git branch --> list the pwd
git config  --global user.name "myname" --> set up username during the initial git set up

SMAC era --> Social media, Mobile, Analytics & cloud

1990's --> www/dot com era, personal computers

Era of Digitization

Data Science Life cycle
-------------------------------
Data Engineering  -->  Collect, store & manage [RDBMS, Nosql db's, Hadoop, Cloud, Data Lakes, Data warehousing]

Data Analysis --> Analyse data using maths, stats & domain knowledge [Python(Pandas), R, SAS, SQL, Pyspark]

Data Visualisation --> Reports, charts, presentations [Excel, Tableau, Power BI, Qlikview, Lookr]

Machine Learning  --> Building predictive models, clusters etc. on the data [Python, Spark ML, R, Mahout]

Descriptive Analytics
Diagnostics Analytics
Predictive Analytics
Prescriptive Analytics

Challenges with Big Data    --> Traditional RDBMS
----------------------------------
Volume --> Cost of storage, [Licensed s/w, High end costly servers], Portability, premature death of the death

Velocity --> Speed of data generation

Variety --> Unstructured data

2005 --> Doug cutting --> Hadoop --> Apache foundation --> Opensource framework --> Java
Storing, processing and managing Big data

Hadoop Vendors --> Apache, Cloudera, Hortonworks, MapR, MS HD Insights, IBM Big Insights

Hadoop --> Opensource, cheap commodity hardware, Unstructured data can be stored, faster processing of data

Hadoop Clusters  ---> Hadoop programs running in multiple machines/servers/nodes
----------------------
Each hadoop server is capable of managing 10 TB data for ex
300*10 --> 3 PB --> 3000 TB

Modes  of cluster configuration --> Standalone, Pseudo distributed[Configured to run on single server], Fully Distributed mode[Configured to run on multiple server]

Characterstics
-------------------
Cost effective, Flexible, Scalable, Fault tolerant

Big Data Ecosystem
--------------------------
Core components

1. HDFS [Hadoop Distributed File System] --> Storage Layer  --> Distributed Architecture
2. Mapreduce 2 [YARN --> Yet Another Resource Negotiator] --> Processing Layer --> Parallel Processing

additional components

Pig --> Adhoc query language (Pig latin), data flow language , Load, Transform(generate, filter, describe, avg) & dump/store (Invented by Yahoo!)

Hive --> Data Warehousing like solution, HQL (FB)

Squoop --> Connecting with other db's

Flume --> Data integration from web

Oozie --> Work Scheduler

Zoo keeper --> Job Co-ordinator

Mahout --> ML library (Java)

HBASE --> No sql db (columnar)


HDFS Architecture   --> Master slave architecture
-------------------------
Processes/daemons of HDFS are,

Master --> Name node --> Maintains & manages the blocks which are present in the data nodes
It keeps track of entire data/ directory structure and also the placement of the blocks
Keep track of meta data of blocks and replications

Slave Machines --> Data nodes --> Data is stored & processed here. Responsible for serving the read/write requests

File storage
----------------
Files are split into physical blocks of 128 mb and further replicated 3 times by default and stored in the data nodes

Replication factor can be set by the Hadoop Admin

Hadoop 1 --> 64 mb
Hadoop 2 -->128 mb

280 mb file

The data is divided into blocks of 128 mb maximum and distributed across the data nodes in parallel.
Replications are written in serial

b1 --> 128 mb
b2 --> 128 mb
b3 --> 44 mb

Name node
----------------
Stores the meta data  --> List of files, list of blocks & its replications for each file, list of data nodes for each blocks, file attributes, access_time, transaction_log etc.

Rack Awareness --> Hadoop makes sure to store the replications in different data nodes/racks/servers.

File Read/Write Anatomy
----------------------------------
When a read/write request is submitted --> Request is taken by the Client library(Hadoop FS)
Interface between Hadoop and the User

hadoop fs  -put /source_path /destination_path

1.  Client takes the request, splits the data into blocks of 128 mb  (maximum) and replications
2. Client collects the list of data nodes details from the "Name node" to write the blocks and replications
3. Blocks are written in parallel, replications are written serially

By default replication factor 3

Limitations of Hadoop 1
----------------------------------
Secondary Name node --> Manual back up of the name node (Single point of failure)
Single Name space --> Limited by the single ram space (Name node)

Hadoop 2
-------------
High Availability --> Active & Standby name node (Auto back-up)
Federation --> Multiple name nodes

/HR --> Name node1
/Sales --> Name node2
/Marketing --> Name node3

Commands
---------------
ls --> To list the files and directories
mkdir --> To create directory
cd --> To change the directory
gedit / vi --> i (insert), right click for paste, (esc+:wq) for quit
cat --> To view the file

Hadoop commands  --> all linux commands to start with hadoop  fs   - 
---------------------------

jps --> To check the currently running daemons

HDFS  --> start-dfs.sh --> To start HDFS daemons
Mapreduce  --> start-yarn.sh --> To start mapreduce daemons
start-all.sh --> To start all the hadoop daemons
stop-all.sh --> To stop all the hadoop daemons
localhost/50070 --> To browse the hadoop file system

hadoop fs  -ls /

hadoop fs  -put      /source_path         /destination_path 

/home/hduser/ubuntu_dir   -->     hadoop fs   -put  test.txt  /hdfs_dir/

/home --> hadoop fs  -put /home/hduser/ubuntu_dir/file.txt  /hdfs_dir/new_test.txt

hadoop fs  -get /source_path /destination_path

Mapreduce / YARN [Yet Another Resource Negotiator]  --> Parallel processing framework
-------------------------------------------------------------------------
MR1 --> Job Tracker --> Master --> Burden (Managing cluster level resources, cpu/ram) scheduling of tasks, managing & monitoring every task

Task Trackers --> Slaves --> Allocation of all the resources for all the tasks

MR2/YARN
---------------
Resource manager (Master) --> Manages all the cluster level resources and scheduling of jobs
Node manager (Slave) --> Manages allocation of the jobs, one present per data node

Application master --> Present for each jobs, Negotiates with Resource Manager to schedule tasks

MR program paradigm
------------------------------

240 mb file

1b --> 128 mb
2b --> 112 mb

Split, distribute and manage  the data running in parallel with fault tolerance.

Map class --> Implements the business logic --> Overrides the map method [select * from employees where sal>10k]

Reduce class --> Aggregation logic --> Overrides the reducer method (count)

Framework itself reads the data from the blocks and gives it to the mappers

Input format class --> Responsible for reading the data from the blocks and giving it to the mappers & reducers

Text input format class --> Text files
Sequence file format --> Binary files
XML input format class --> XML files

/ * Word count program */

1. Mapper Input 

s = "Hi, we are in python class, python is awesome!"
s.split(" ")
for i in l:
     print(i,1)

hi,1
we,1
are,1
python,1
class,1
python,1
is,1
awesome,1

2. Combiner (Mini-reducer) --> (hi,1) (we,1) (are,1) (python,1,1) (class,1) (is,1) (awesome,1)
3. Reducer Input --> Same as the mapper's output
4. Reducer output --> Aggregation logic (take the sum of the values for each keys and pass it to the same reducer output)
(hi,1), (python,2)
5  Partitioner --> Responsible for sending the output of the same key to the same output directory

Print the number of emp who are getting salary > 10k in each cities

Blr --> 100
Mum --> 80
Che --> 60

Map task --> Input splits --> Num_mappers

Get data from input splits
process
produce the intermediate temporary output

Reduce task --> (By default is 1)

Reads from all the map tasks
processs
output is stored in the hdfs in blocks and replications

Print the  names of employees getting more than 10k salary? --> No aggregation is required and hence no reducer is needed
Count the number of employees getting > 10k sal? --> reducer is needed to count(aggregation)

hadoopy, pydoop, mrjob

Hadoop Streaming
--------------------------
Utility that comes along with the hadoop distribution which helps in running executable scripts of Python or R

/usr/local/hadoop-2.9.1/share/hadoop/tools/lib

chmod a+x / chmod 777 (give permission to your mapper & reducer scripts)

chmod a+x mapper.py reducer.py

#! --> Shabong --> To run a executable python script
#!/usr/bin/python

 hadoop jar /usr/local/hadoop-2.9.1/share/hadoop/tools/lib/hadoop-streaming-2.9.1.jar  -input /Pavan/myfile.txt -output /Pavan/wordcount2 -mapper /home/hduser/Pavan/mapper1.py  -reducer /home/hduser/Pavan/reducer1.py

1. Make sure the #! "Excutable script path is given"
2. verify the python syntax
3. Give permission to mapper & reducer file
4. Ensure the path of the hadoop streaming jar, input & output files, mapper & reducer files.

1st question
==========
4 --> 4 patients
126 140 160 150 --> bp
150 160 170 140 --> blood glucose

avgbg=bp+gl/2 
np.mean(avgbg) > 135 --> 'unhealthy'

2nd question
===========
take a list as input   --> [a,b,c,d,e,f,g,h,i,j]
take another one --> [2,4,6]
print a series for the position given there

3rd question
===========
take an input as list
take a string as input
asssign string as the name of the list
take another list as index for the list taken before
and print the list with index and the name along with dtype

0   a
1   b
2   c
  dtype:str

Hive
====
select * from employee where sal>10000

DWH --> OLAP
DB --> OLTP
 
Data warehousing like solution 
invented by FB

select * from data where people liked>100; 

(100 lines of code --> java)

(50 lines of code --> python)

Lot of data
Streaming data
Variety of data

With MR --> Hard to code
Solution --> They had a lot of inhouse sql developers

HQL--> Hive Query Language (Sql like)

Thrift servers --> JDBC/ODBC --> enable cross platform application development

oracle sql --> locally + Azure sql on cloud
Hive/sql + AWS redshift
sql + az synapse

Metastore / derby --> Local database of hive --> Metadata about the data in Hive
Hue --> Web interface for hive --> Only available in cloudera

Hive is schema on read / sql is schema on write

create table employees(name string, salary float, dept string)
row format delimited
fields terminated by ',';

// If the second line is not given (newline is assumed to be row delimiter)
//third line by default ascii is considered to be the column seperator

describe formatted database deloitte;
load data local inpath 'emp1.txt' into table employees;
describe formatted employees;

Internal table/managed table
========================
Upon loading a data from hdfs into internal table, the data gets moved from hdfs location to hive warehouse location inside hdfs
Now, data is only accessible to people working on hive, making it un accessible from anywhere else.

If the table is dropped, data is lost permanently
 
External tables
============
create external table employees(name string, salary float, dept string)
row format delimited
fields terminated by ','
location '/hdfs_july24/hive';

describe formatted employees;

The table is mapped into the location of hdfs, the data stays intact irrespective of whatever we do in the table

Partitions
========
create table employees(name string, salary float, dept string)
partitioned by (state string)
row format delimited
fields terminated by ','

load data local inpath 'emp.txt' into table employees
partition (state="KA")

load data local inpath 'emp1.txt' into table employees
partition (state="MH")

select * from employees
show partitions employees
select * from employees where state='KA' --> data is fetched from metastore
select avg(salary) from employees; --> this triggers a MR job

alter table employees drop if exists partition(state='Dl')
insert overwrite directory 'hdfs_july24' select * from employees where dept="HR"

Partition for external table
======================
create table employees(name string, salary float, dept string)
partitioned by (state string)
row format delimited
fields terminated by ','
location '/hdfs/hive';

alter table employees
add partition(state='KA')
location '/hdfs/hive/Ka'

show partitions employees
alter table employees drop if exists partition (state=string)
show partitions employees

static partitioning
===============
create table master(name string, sal float, dept string)
row format delimited
fields terminated by ','

load data local inpath 'emp*txt' into table master

create table static(name string, sal float)
partitioned by(dept string)
row format delimited
fields terminated by ',';

insert into table static partition(dept="HR") select name, sal from master where dept="HR";
insert into table static partition(dept="MK") select name, sal from master where dept="MK";
insert into table static partition(dept="FI") select name, sal from master where dept="FI";

dynamic partitioning

create table dynamic(name string, sal float)
partitioned by(dept string)
row format delimited
fields terminated by ',';

set hive.exec.dynamic.partition.mode=nonstrict
insert into table static partition(dept) select name, sal,dept from master

Bucketing
========
create table bucks(name string, sal float)
#partitioned by(dept string)
clustered by (sal) into 2 buckets
row format delimited
fields terminated by ',';

set hive.enforce.bucketing=true
set hive.enforce.sorting = true

insert into table bucks partition(dept) select name, sal,dept from master

Spark
=====
*-------------------------------------------------------------------------------------*
Spark --> Unified, in-memory [RAM] cluster computing framework    [written in Scala (functional programming language)] --> 100 times faster than YARN (in-memory) 10 times faster than YARN (in disc)

Unified platform 
==============
Complex programming logic (RDD syntax to write programs) using Scala, Python, Java, R
Dataframes
spark sql
spark ml, mllib
spark streaming 
graph

Run anywhere
============
HDFS, Apache mesos, AWS S3, Azure blob storage

Spark RDD (Resilient Distributed Datasets) --> Logical Split of the data

Lazy evalution
============
Transformations --> The functions or business logic --> Upon applying a transformation
spark creates a logical plan of execution (Directed Acyclic Graphs [DAG's])

Actions --> Actual execution of the logical plan

Narrow Transformations 
Wide Transformations 

Shared Variables --> Variables that has to be shared among different RDD's 

Accumulators
Broadcasters

Spark Persistance  --> Save the execution state

Spark SQL dataframes


Spark MLlib --> RDD's (outdated)
Spark ML --> Spark Dataframes

Spark Streaming

Kafka

Pyspark
-----------
RDD syntax  --> Spark Context
Dataframes --> Spark Session --> Spark context, streaming context, sql context
Datasets --> Not available for Python/Java


RDD's exposes API's called as Transformations and Actions

Transformations : Takes one RDD as input and produce another RDD as output

1. Row level transformations : Map, Filter, Flatmap, map partition, map partition with index

2. Aggregations : Reduce bykey,aggregate bykey

3. Joins

4. Ranking: groupbykey

Actions : They produce the final output to the driver program

1. Preview --> Take,Sample,top

2. Collect

3. Reduce

4. Saveastext,saveasseq

# Directed Acyclic Graphs : 

# Dag Scheduler : It completes the computation and execution of stages for a job. Tracks the RDD's--> Assign task to task scheduler

# Task scheduler ( Submitting tasks for execution)

# DAG process:

1. User submits an apache spark application

2. The driver module takes the application (Spark context is set-up)

3. Driver identify transformations & Actions required

4. Creates a logical flow of operations (DAG)

5. Dag is converted into Physical execution plan ( Stages )

6. Then the tasks are sent to the cluster with the help of DAG scheduler


Parquet file format
---------------------------

Compressed columnar file format


Machine Learning
-------------------------

Supervised 
---------------

Building predictive models  --> Learning from the statistical/mathematical relationship between Y var & all the x vars.

Target/Dependent variable  (y var) 
Predictor / Independent variables (x vars)

classification --> Target variable is pre-defined
Regression --> Target variable is a entire range of values / continuous

Linear models --> When y &  all the x vars relationship is simple
Non-linear --> When y &  all the x vars relationship is complex
Ensemble --> Combination of more than one model's learning (Bagging, Boosting)

Regression
----------------
Linear Regression 
y = mx+c
e(Price) =  b0+ b1*(CRIM)+b2*(LSTAT)+b3*(RM)+...............+e

OLS
Line of best fit
Residual / Error --> Squared difference between actual & fitted values
Constant / Intercept --> A base value when X is "0"
Slope coefficient / weights --> Impact of x on y (variation of Y caused by variation of X)

Standardisation -->  Center & Scale

((xi-mean(x) / std(x))

Classification
------------------






















